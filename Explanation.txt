# Importing necessary libraries
"In this step, we begin by importing the essential libraries required for building our sales prediction model. These include Pandas and NumPy for handling and analyzing data, Matplotlib and Seaborn for creating insightful visualizations, and Scikit-learn for machine learning operations. Specifically, we use Linear Regression and Random Forest Regressor to build individual models, and Voting Regressor to combine their predictions into a single robust model. Additionally, we import train-test splitting tools and metrics like Mean Absolute Error and Mean Squared Error to evaluate the performance of our models. With these tools in place, we’re ready to dive into the dataset."

# Load the dataset
"In this step, we load the dataset containing advertising spend data across different channels, along with corresponding sales figures. The dataset is read using the Pandas library from the specified file path, ensuring it’s ready for further analysis and preprocessing."

# Exploratory Data Analysis (EDA)
"Here, we perform exploratory data analysis to understand the structure and content of our dataset. First, we display the first five rows to get a quick preview of the data. Then, we use the `info` method to examine the dataset's structure, including column names, data types, and any missing values. Finally, we generate summary statistics to analyze key metrics such as mean, minimum, and maximum values for each numerical column. This helps us identify patterns and understand the dataset's overall characteristics."

------------------------------------------------

# Checking for missing values
"In this step, we check for any missing values in the dataset using the `isnull` function. This ensures data integrity by identifying columns with incomplete data. The output shows the number of missing values in each column, which is crucial for determining if further data cleaning is required."

# Data Cleaning: Dropping rows with missing values
"To ensure our dataset is clean and reliable, we remove any rows with missing values using the `dropna` function. This step eliminates incomplete data, which could otherwise affect the accuracy and performance of our machine learning models."

# Visualizations
"In this step, we visualize the relationships between variables using a correlation heatmap. By calculating the correlation coefficients, we identify how strongly each variable is related to others, especially the target variable, 'Sales.' The heatmap, created with Seaborn, uses a color gradient to represent these correlations, making it easier to spot key features for model building. This helps us focus on the variables most likely to influence sales."

# Feature Selection and Target Variable
"Next, we perform feature selection by separating our dataset into independent and dependent variables. The independent variables, or features, are stored in `X`, excluding the 'Sales' column, while the dependent variable, which we aim to predict, is stored in `y` as the 'Sales' column. This separation is essential for training our machine learning models."

# Encoding categorical variables (if any)
"In this step, we handle any categorical variables in the dataset by encoding them into numerical values using one-hot encoding. The `pd.get_dummies` function converts categorical columns into binary columns, ensuring our data is compatible with machine learning algorithms. The `drop_first` parameter avoids multicollinearity by removing one dummy variable from each category."

# Splitting the data into training and testing sets
"Now, we split the dataset into training and testing sets using an 80-20 ratio. This means 80% of the data is used for training the model, while 20% is reserved for testing its performance. The `train_test_split` function also ensures randomness in the split while maintaining reproducibility with a fixed `random_state`. This separation is crucial for evaluating the model's accuracy on unseen data."

# Linear Regression Model
"At this stage, we define two machine learning models for predicting sales. First, we initialize a `LinearRegression` model, which is a simple yet effective algorithm for understanding linear relationships between features and the target variable. Next, we set up a `RandomForestRegressor`, a powerful ensemble learning method that uses multiple decision trees to make more accurate and robust predictions. By combining these models later, we leverage their individual strengths to enhance performance."

# Combined Model using Voting Regressor
"Here, we create a combined model using the `VotingRegressor`, which blends the predictions of multiple models to improve accuracy. We include both the Linear Regression and Random Forest models as estimators in the Voting Regressor. By fitting this combined model to the training data, we enable it to leverage the strengths of each individual model, resulting in a more balanced and reliable prediction system."

# Predictions and Evaluation
"Once the combined model is trained, we use it to make predictions on the test data. To evaluate its performance, we calculate key metrics such as Mean Absolute Error, Mean Squared Error, and Root Mean Squared Error. These metrics give us insight into the accuracy of the model by measuring the average difference between the predicted and actual sales values. Lower error values indicate better model performance."

# Feature Importance (for Random Forest)
"In this step, we analyze the importance of each feature in predicting sales using the Random Forest model. After fitting the Random Forest model to the training data, we extract the feature importances, which indicate how much each variable contributes to the predictions. We visualize this information using a bar plot, where the features are ranked in descending order of importance. This analysis helps us understand which variables have the most significant impact on sales, guiding us in feature selection and optimization."
